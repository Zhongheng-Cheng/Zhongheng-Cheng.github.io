---
layout: post
---

<!-- ## Difficulties

## Ideas

## Challenges

## Attempts to succeed

## Failures

## Advice -->

## Finished

### Llama2 + llama-index [[Source](https://docs.llamaindex.ai/en/stable/examples/vector_stores/SimpleIndexDemoLlama-Local/)]

Last week, we tried llama-index RAG ability with its default ChatGPT-3.5-Turbo model. This time we will integrate llama2 model into the llama-index framework to enhance llama2's RAG capability.

We can call and download the llama2 models with its weight data from `Huggingface`, and this can be done directly with llama-index library as the `transformers` library has been wrapped as `llama_index.llms.HuggingFaceLLM`, according to [LlamaIndex - Huggingface LLMs](https://docs.llamaindex.ai/en/stable/examples/llm/huggingface/). Also, this method helps ensuring that llama2 model runs on GPU, which saves time up to 90% (it takes around 10 mins to run a one-round conversation on CPU). 

The workflow is similar to the previous RAG process, except that we will create a llama2 model first, setup all of its parameters, and pass it to llama-index so that the `VectorStoreIndex` component of llama-index would be able to use llama2 as its `llm`.

## Other Messages From The Weekly Meeting

...

## To-do List For Next Week

...

## Weekly Meeting Slides Link

...